{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>capacity_bytes</th>\n",
       "      <th>date</th>\n",
       "      <th>days_before_failure</th>\n",
       "      <th>failure</th>\n",
       "      <th>model</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>smart_10_raw</th>\n",
       "      <th>smart_11_raw</th>\n",
       "      <th>...</th>\n",
       "      <th>smart_251_raw</th>\n",
       "      <th>smart_252_raw</th>\n",
       "      <th>smart_254_raw</th>\n",
       "      <th>smart_2_raw</th>\n",
       "      <th>smart_3_raw</th>\n",
       "      <th>smart_4_raw</th>\n",
       "      <th>smart_5_raw</th>\n",
       "      <th>smart_7_raw</th>\n",
       "      <th>smart_8_raw</th>\n",
       "      <th>smart_9_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>312.0</td>\n",
       "      <td>0</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>S301GMGW</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>-0.118116</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>311.0</td>\n",
       "      <td>0</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>S301GMGW</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>-0.118116</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>S301GMGW</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>-0.118116</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>309.0</td>\n",
       "      <td>0</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>S301GMGW</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>-0.118116</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.000787e+12</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>308.0</td>\n",
       "      <td>0</td>\n",
       "      <td>ST4000DM000</td>\n",
       "      <td>S301GMGW</td>\n",
       "      <td>-0.012181</td>\n",
       "      <td>-0.118116</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  capacity_bytes        date  days_before_failure  \\\n",
       "0           0             0    4.000787e+12  2018-01-01                312.0   \n",
       "1           1             1    4.000787e+12  2018-01-02                311.0   \n",
       "2           2             2    4.000787e+12  2018-01-03                310.0   \n",
       "3           3             3    4.000787e+12  2018-01-04                309.0   \n",
       "4           4             4    4.000787e+12  2018-01-05                308.0   \n",
       "\n",
       "   failure        model serial_number  smart_10_raw  smart_11_raw  ...  \\\n",
       "0        0  ST4000DM000      S301GMGW     -0.012181     -0.118116  ...   \n",
       "1        0  ST4000DM000      S301GMGW     -0.012181     -0.118116  ...   \n",
       "2        0  ST4000DM000      S301GMGW     -0.012181     -0.118116  ...   \n",
       "3        0  ST4000DM000      S301GMGW     -0.012181     -0.118116  ...   \n",
       "4        0  ST4000DM000      S301GMGW     -0.012181     -0.118116  ...   \n",
       "\n",
       "   smart_251_raw  smart_252_raw  smart_254_raw  smart_2_raw  smart_3_raw  \\\n",
       "0            NaN            NaN            NaN          NaN          NaN   \n",
       "1            NaN            NaN            NaN          NaN          NaN   \n",
       "2            NaN            NaN            NaN          NaN          NaN   \n",
       "3            NaN            NaN            NaN          NaN          NaN   \n",
       "4            NaN            NaN            NaN          NaN          NaN   \n",
       "\n",
       "   smart_4_raw  smart_5_raw  smart_7_raw  smart_8_raw  smart_9_raw  \n",
       "0          NaN          NaN          NaN          NaN          NaN  \n",
       "1          NaN          NaN          NaN          NaN          NaN  \n",
       "2          NaN          NaN          NaN          NaN          NaN  \n",
       "3          NaN          NaN          NaN          NaN          NaN  \n",
       "4          NaN          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./Data/final_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({312.0: 318,\n",
       "         311.0: 330,\n",
       "         310.0: 340,\n",
       "         309.0: 355,\n",
       "         308.0: 360,\n",
       "         307.0: 359,\n",
       "         306.0: 364,\n",
       "         305.0: 365,\n",
       "         304.0: 372,\n",
       "         303.0: 389,\n",
       "         302.0: 394,\n",
       "         301.0: 404,\n",
       "         300.0: 415,\n",
       "         299.0: 417,\n",
       "         298.0: 423,\n",
       "         297.0: 433,\n",
       "         296.0: 437,\n",
       "         295.0: 445,\n",
       "         294.0: 452,\n",
       "         293.0: 458,\n",
       "         292.0: 461,\n",
       "         291.0: 467,\n",
       "         290.0: 478,\n",
       "         289.0: 472,\n",
       "         288.0: 479,\n",
       "         287.0: 485,\n",
       "         286.0: 489,\n",
       "         285.0: 499,\n",
       "         284.0: 508,\n",
       "         283.0: 514,\n",
       "         282.0: 519,\n",
       "         281.0: 527,\n",
       "         280.0: 534,\n",
       "         279.0: 537,\n",
       "         278.0: 541,\n",
       "         277.0: 553,\n",
       "         276.0: 560,\n",
       "         275.0: 570,\n",
       "         274.0: 580,\n",
       "         273.0: 592,\n",
       "         272.0: 593,\n",
       "         271.0: 600,\n",
       "         270.0: 599,\n",
       "         269.0: 601,\n",
       "         268.0: 610,\n",
       "         267.0: 619,\n",
       "         266.0: 631,\n",
       "         265.0: 638,\n",
       "         264.0: 645,\n",
       "         263.0: 647,\n",
       "         262.0: 656,\n",
       "         261.0: 661,\n",
       "         260.0: 668,\n",
       "         259.0: 679,\n",
       "         258.0: 687,\n",
       "         257.0: 686,\n",
       "         256.0: 693,\n",
       "         255.0: 702,\n",
       "         254.0: 707,\n",
       "         253.0: 719,\n",
       "         252.0: 736,\n",
       "         251.0: 741,\n",
       "         250.0: 753,\n",
       "         249.0: 767,\n",
       "         248.0: 775,\n",
       "         247.0: 783,\n",
       "         246.0: 789,\n",
       "         245.0: 795,\n",
       "         244.0: 798,\n",
       "         243.0: 808,\n",
       "         242.0: 819,\n",
       "         241.0: 824,\n",
       "         240.0: 828,\n",
       "         239.0: 836,\n",
       "         238.0: 839,\n",
       "         237.0: 847,\n",
       "         236.0: 850,\n",
       "         235.0: 855,\n",
       "         234.0: 860,\n",
       "         233.0: 867,\n",
       "         232.0: 876,\n",
       "         231.0: 885,\n",
       "         230.0: 891,\n",
       "         229.0: 894,\n",
       "         228.0: 902,\n",
       "         227.0: 911,\n",
       "         226.0: 916,\n",
       "         225.0: 929,\n",
       "         224.0: 937,\n",
       "         223.0: 937,\n",
       "         222.0: 938,\n",
       "         221.0: 947,\n",
       "         220.0: 958,\n",
       "         219.0: 976,\n",
       "         218.0: 986,\n",
       "         217.0: 1000,\n",
       "         216.0: 1001,\n",
       "         215.0: 998,\n",
       "         214.0: 1009,\n",
       "         213.0: 1019,\n",
       "         212.0: 1024,\n",
       "         211.0: 1037,\n",
       "         210.0: 1044,\n",
       "         209.0: 1050,\n",
       "         208.0: 1053,\n",
       "         207.0: 1060,\n",
       "         206.0: 1077,\n",
       "         205.0: 1090,\n",
       "         204.0: 1103,\n",
       "         203.0: 1109,\n",
       "         202.0: 1118,\n",
       "         201.0: 1130,\n",
       "         200.0: 1130,\n",
       "         199.0: 1132,\n",
       "         198.0: 1138,\n",
       "         197.0: 1142,\n",
       "         196.0: 1144,\n",
       "         195.0: 1155,\n",
       "         194.0: 1164,\n",
       "         193.0: 1183,\n",
       "         192.0: 1187,\n",
       "         191.0: 1192,\n",
       "         190.0: 1206,\n",
       "         189.0: 1210,\n",
       "         188.0: 1224,\n",
       "         187.0: 1239,\n",
       "         186.0: 1249,\n",
       "         185.0: 1257,\n",
       "         184.0: 1258,\n",
       "         183.0: 1266,\n",
       "         182.0: 1274,\n",
       "         181.0: 1277,\n",
       "         180.0: 1278,\n",
       "         179.0: 1292,\n",
       "         178.0: 1294,\n",
       "         177.0: 1298,\n",
       "         176.0: 1306,\n",
       "         175.0: 1309,\n",
       "         174.0: 1312,\n",
       "         173.0: 1326,\n",
       "         172.0: 1338,\n",
       "         171.0: 1348,\n",
       "         170.0: 1356,\n",
       "         169.0: 1365,\n",
       "         168.0: 1374,\n",
       "         167.0: 1378,\n",
       "         166.0: 1381,\n",
       "         165.0: 1387,\n",
       "         164.0: 1396,\n",
       "         163.0: 1408,\n",
       "         162.0: 1422,\n",
       "         161.0: 1431,\n",
       "         160.0: 1437,\n",
       "         159.0: 1445,\n",
       "         158.0: 1453,\n",
       "         157.0: 1464,\n",
       "         156.0: 1474,\n",
       "         155.0: 1488,\n",
       "         154.0: 1501,\n",
       "         153.0: 1506,\n",
       "         152.0: 1508,\n",
       "         151.0: 1513,\n",
       "         150.0: 1517,\n",
       "         149.0: 1518,\n",
       "         148.0: 1527,\n",
       "         147.0: 1535,\n",
       "         146.0: 1537,\n",
       "         145.0: 1539,\n",
       "         144.0: 1545,\n",
       "         143.0: 1553,\n",
       "         142.0: 1564,\n",
       "         141.0: 1581,\n",
       "         140.0: 1590,\n",
       "         139.0: 1594,\n",
       "         138.0: 1606,\n",
       "         137.0: 1617,\n",
       "         136.0: 1629,\n",
       "         135.0: 1638,\n",
       "         134.0: 1642,\n",
       "         133.0: 1649,\n",
       "         132.0: 1660,\n",
       "         131.0: 1662,\n",
       "         130.0: 1667,\n",
       "         129.0: 1682,\n",
       "         128.0: 1688,\n",
       "         127.0: 1699,\n",
       "         126.0: 1708,\n",
       "         125.0: 1712,\n",
       "         124.0: 1726,\n",
       "         123.0: 1740,\n",
       "         122.0: 1747,\n",
       "         121.0: 1753,\n",
       "         120.0: 1759,\n",
       "         119.0: 1770,\n",
       "         118.0: 1789,\n",
       "         117.0: 1798,\n",
       "         116.0: 1809,\n",
       "         115.0: 1821,\n",
       "         114.0: 1832,\n",
       "         113.0: 1840,\n",
       "         112.0: 1851,\n",
       "         111.0: 1862,\n",
       "         110.0: 1874,\n",
       "         109.0: 1880,\n",
       "         108.0: 1894,\n",
       "         107.0: 1904,\n",
       "         106.0: 1916,\n",
       "         105.0: 1931,\n",
       "         104.0: 1945,\n",
       "         103.0: 1956,\n",
       "         102.0: 1961,\n",
       "         101.0: 1981,\n",
       "         100.0: 1977,\n",
       "         99.0: 1993,\n",
       "         98.0: 2005,\n",
       "         97.0: 2002,\n",
       "         96.0: 2011,\n",
       "         95.0: 2022,\n",
       "         94.0: 2040,\n",
       "         93.0: 2046,\n",
       "         92.0: 2053,\n",
       "         91.0: 2067,\n",
       "         90.0: 2067,\n",
       "         89.0: 2066,\n",
       "         88.0: 2071,\n",
       "         87.0: 2073,\n",
       "         86.0: 2081,\n",
       "         85.0: 2090,\n",
       "         84.0: 2102,\n",
       "         83.0: 2110,\n",
       "         82.0: 2120,\n",
       "         81.0: 2125,\n",
       "         80.0: 2125,\n",
       "         79.0: 2136,\n",
       "         78.0: 2137,\n",
       "         77.0: 2143,\n",
       "         76.0: 2142,\n",
       "         75.0: 2146,\n",
       "         74.0: 2151,\n",
       "         73.0: 2152,\n",
       "         72.0: 2163,\n",
       "         71.0: 2167,\n",
       "         70.0: 2172,\n",
       "         69.0: 2178,\n",
       "         68.0: 2188,\n",
       "         67.0: 2195,\n",
       "         66.0: 2201,\n",
       "         65.0: 2210,\n",
       "         64.0: 2215,\n",
       "         63.0: 2217,\n",
       "         62.0: 2221,\n",
       "         61.0: 2236,\n",
       "         60.0: 2250,\n",
       "         59.0: 2260,\n",
       "         58.0: 2265,\n",
       "         57.0: 2270,\n",
       "         56.0: 2274,\n",
       "         55.0: 2275,\n",
       "         54.0: 2288,\n",
       "         53.0: 2306,\n",
       "         52.0: 2313,\n",
       "         51.0: 2332,\n",
       "         50.0: 2343,\n",
       "         49.0: 2354,\n",
       "         48.0: 2372,\n",
       "         47.0: 2369,\n",
       "         46.0: 2386,\n",
       "         45.0: 2388,\n",
       "         44.0: 2380,\n",
       "         43.0: 2397,\n",
       "         42.0: 2415,\n",
       "         41.0: 2420,\n",
       "         40.0: 2435,\n",
       "         39.0: 2449,\n",
       "         38.0: 2462,\n",
       "         37.0: 2470,\n",
       "         36.0: 2489,\n",
       "         35.0: 2499,\n",
       "         34.0: 2496,\n",
       "         33.0: 2505,\n",
       "         32.0: 2508,\n",
       "         31.0: 2514,\n",
       "         30.0: 2521,\n",
       "         29.0: 2527,\n",
       "         28.0: 2540,\n",
       "         27.0: 2550,\n",
       "         26.0: 2585,\n",
       "         25.0: 2601,\n",
       "         24.0: 2623,\n",
       "         23.0: 2626,\n",
       "         22.0: 2638,\n",
       "         21.0: 2653,\n",
       "         20.0: 2673,\n",
       "         19.0: 2689,\n",
       "         18.0: 2708,\n",
       "         17.0: 2732,\n",
       "         15.0: 2758,\n",
       "         14.0: 2776,\n",
       "         13.0: 2790,\n",
       "         12.0: 2808,\n",
       "         11.0: 2828,\n",
       "         10.0: 2851,\n",
       "         9.0: 2860,\n",
       "         8.0: 2869,\n",
       "         7.0: 2888,\n",
       "         6.0: 2908,\n",
       "         5.0: 2943,\n",
       "         4.0: 2984,\n",
       "         3.0: 3018,\n",
       "         2.0: 3045,\n",
       "         1.0: 3078,\n",
       "         0.0: 3162,\n",
       "         16.0: 2752,\n",
       "         343.0: 125,\n",
       "         342.0: 131,\n",
       "         341.0: 136,\n",
       "         340.0: 150,\n",
       "         339.0: 159,\n",
       "         338.0: 165,\n",
       "         337.0: 166,\n",
       "         336.0: 166,\n",
       "         335.0: 173,\n",
       "         334.0: 185,\n",
       "         333.0: 195,\n",
       "         332.0: 199,\n",
       "         331.0: 209,\n",
       "         330.0: 212,\n",
       "         329.0: 222,\n",
       "         328.0: 228,\n",
       "         327.0: 226,\n",
       "         326.0: 239,\n",
       "         325.0: 247,\n",
       "         324.0: 253,\n",
       "         323.0: 265,\n",
       "         322.0: 271,\n",
       "         321.0: 276,\n",
       "         320.0: 280,\n",
       "         319.0: 290,\n",
       "         318.0: 293,\n",
       "         317.0: 289,\n",
       "         316.0: 298,\n",
       "         315.0: 302,\n",
       "         314.0: 306,\n",
       "         313.0: 307,\n",
       "         355.0: 45,\n",
       "         354.0: 51,\n",
       "         353.0: 54,\n",
       "         352.0: 62,\n",
       "         351.0: 69,\n",
       "         350.0: 78,\n",
       "         349.0: 80,\n",
       "         348.0: 82,\n",
       "         347.0: 83,\n",
       "         346.0: 89,\n",
       "         345.0: 96,\n",
       "         344.0: 108,\n",
       "         357.0: 32,\n",
       "         356.0: 35,\n",
       "         363.0: 6,\n",
       "         362.0: 9,\n",
       "         361.0: 13,\n",
       "         360.0: 16,\n",
       "         359.0: 20,\n",
       "         358.0: 21,\n",
       "         364.0: 3,\n",
       "         1000.0: 3053624})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.fillna(0)\n",
    "from collections import Counter\n",
    "#ounter(list(data.loc[:, 'days_before_failure'].values))\n",
    "#final_data = data[data['days_before_failure'] != 119]\n",
    "final_data = data[data['days_before_failure'] >= -1.0]\n",
    "final_data[final_data['days_before_failure'] == 0] = 1000\n",
    "final_data[final_data['days_before_failure'] == -1] = 0\n",
    "Counter(list(final_data.loc[:, 'days_before_failure'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smart_5_raw</th>\n",
       "      <th>smart_9_raw</th>\n",
       "      <th>smart_187_raw</th>\n",
       "      <th>smart_188_raw</th>\n",
       "      <th>smart_193_raw</th>\n",
       "      <th>smart_194_raw</th>\n",
       "      <th>smart_197_raw</th>\n",
       "      <th>smart_198_raw</th>\n",
       "      <th>smart_241_raw</th>\n",
       "      <th>smart_242_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.014346</td>\n",
       "      <td>-0.067059</td>\n",
       "      <td>0.547236</td>\n",
       "      <td>0.085230</td>\n",
       "      <td>-0.019682</td>\n",
       "      <td>-0.019178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.014346</td>\n",
       "      <td>-0.067059</td>\n",
       "      <td>0.547236</td>\n",
       "      <td>-0.067508</td>\n",
       "      <td>-0.019682</td>\n",
       "      <td>-0.019178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.014346</td>\n",
       "      <td>-0.067059</td>\n",
       "      <td>0.547236</td>\n",
       "      <td>0.237968</td>\n",
       "      <td>-0.019682</td>\n",
       "      <td>-0.019178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.014346</td>\n",
       "      <td>-0.067059</td>\n",
       "      <td>0.547253</td>\n",
       "      <td>0.085230</td>\n",
       "      <td>-0.019682</td>\n",
       "      <td>-0.019178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.014346</td>\n",
       "      <td>-0.067059</td>\n",
       "      <td>0.559559</td>\n",
       "      <td>0.085230</td>\n",
       "      <td>-0.019682</td>\n",
       "      <td>-0.019178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   smart_5_raw  smart_9_raw  smart_187_raw  smart_188_raw  smart_193_raw  \\\n",
       "0          0.0          0.0      -0.014346      -0.067059       0.547236   \n",
       "1          0.0          0.0      -0.014346      -0.067059       0.547236   \n",
       "2          0.0          0.0      -0.014346      -0.067059       0.547236   \n",
       "3          0.0          0.0      -0.014346      -0.067059       0.547253   \n",
       "4          0.0          0.0      -0.014346      -0.067059       0.559559   \n",
       "\n",
       "   smart_194_raw  smart_197_raw  smart_198_raw  smart_241_raw  smart_242_raw  \n",
       "0       0.085230      -0.019682      -0.019178            0.0            0.0  \n",
       "1      -0.067508      -0.019682      -0.019178            0.0            0.0  \n",
       "2       0.237968      -0.019682      -0.019178            0.0            0.0  \n",
       "3       0.085230      -0.019682      -0.019178            0.0            0.0  \n",
       "4       0.085230      -0.019682      -0.019178            0.0            0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Excluding days_before_failure, failure, model, serial_number, index, and date from the training dataset\n",
    "train = final_data.loc[:, data.columns != 'days_before_failure']\n",
    "train = train.loc[:, train.columns != 'failure']\n",
    "train = train.loc[:, train.columns != 'model']\n",
    "train = train.loc[:, train.columns != 'serial_number']\n",
    "train = train.iloc[:, 4:]\n",
    "\n",
    "# Feature Selection: 5, 9, 187, 188, 193, 194, 197, and 198, 241, 242\n",
    "train = train.fillna(0)\n",
    "train = train[['smart_5_raw','smart_9_raw','smart_187_raw',\n",
    "       'smart_188_raw','smart_193_raw','smart_194_raw',\n",
    "       'smart_197_raw','smart_198_raw','smart_241_raw','smart_242_raw']]\n",
    "pd.set_option('max_columns', 300)\n",
    "display(train.head())\n",
    "\n",
    "X = train.iloc[:, :].values\n",
    "y = final_data.loc[:, 'days_before_failure'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from imblearn.over_sampling import SMOTE\n",
    "\n",
    "class_num = dict.fromkeys(y)\n",
    "for cl in class_num:\n",
    "    class_num[cl] = 10000\n",
    "del class_num[0.0]\n",
    "\n",
    "sm = SMOTE(random_state=2, sampling_strategy=class_num, n_jobs=-1)\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
    "print(len(X_train_res))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.    0.    0. ...    0.    0.    0.]\n",
      " [   0.    0.    0. ...    0.    0.    0.]\n",
      " [   0.    0.    0. ...    0.    0.    0.]\n",
      " ...\n",
      " [1000. 1000. 1000. ... 1000. 1000. 1000.]\n",
      " [1000. 1000. 1000. ... 1000. 1000. 1000.]\n",
      " [1000. 1000. 1000. ... 1000. 1000. 1000.]]\n",
      "Counter({1000.0: 100000, 0.0: 3162, 1.0: 3078, 2.0: 3045, 3.0: 3018, 4.0: 2984, 5.0: 2943, 6.0: 2908, 7.0: 2888, 8.0: 2869, 9.0: 2860, 10.0: 2851, 11.0: 2828, 12.0: 2808, 13.0: 2790, 14.0: 2776, 15.0: 2758, 16.0: 2752, 17.0: 2732, 18.0: 2708, 19.0: 2689, 20.0: 2673, 21.0: 2653, 22.0: 2638, 23.0: 2626, 24.0: 2623, 25.0: 2601, 26.0: 2585, 27.0: 2550, 28.0: 2540, 29.0: 2527, 30.0: 2521, 31.0: 2514, 32.0: 2508, 33.0: 2505, 35.0: 2499, 34.0: 2496, 36.0: 2489, 37.0: 2470, 38.0: 2462, 39.0: 2449, 40.0: 2435, 41.0: 2420, 42.0: 2415, 43.0: 2397, 45.0: 2388, 46.0: 2386, 44.0: 2380, 48.0: 2372, 47.0: 2369, 49.0: 2354, 50.0: 2343, 51.0: 2332, 52.0: 2313, 53.0: 2306, 54.0: 2288, 55.0: 2275, 56.0: 2274, 57.0: 2270, 58.0: 2265, 59.0: 2260, 60.0: 2250, 61.0: 2236, 62.0: 2221, 63.0: 2217, 64.0: 2215, 65.0: 2210, 66.0: 2201, 67.0: 2195, 68.0: 2188, 69.0: 2178, 70.0: 2172, 71.0: 2167, 72.0: 2163, 73.0: 2152, 74.0: 2151, 75.0: 2146, 77.0: 2143, 76.0: 2142, 78.0: 2137, 79.0: 2136, 80.0: 2125, 81.0: 2125, 82.0: 2120, 83.0: 2110, 84.0: 2102, 85.0: 2090, 86.0: 2081, 87.0: 2073, 88.0: 2071, 90.0: 2067, 91.0: 2067, 89.0: 2066, 92.0: 2053, 93.0: 2046, 94.0: 2040, 95.0: 2022, 96.0: 2011, 98.0: 2005, 97.0: 2002, 99.0: 1993, 101.0: 1981, 100.0: 1977, 102.0: 1961, 103.0: 1956, 104.0: 1945, 105.0: 1931, 106.0: 1916, 107.0: 1904, 108.0: 1894, 109.0: 1880, 110.0: 1874, 111.0: 1862, 112.0: 1851, 113.0: 1840, 114.0: 1832, 115.0: 1821, 116.0: 1809, 117.0: 1798, 118.0: 1789, 119.0: 1770, 120.0: 1759, 121.0: 1753, 122.0: 1747, 123.0: 1740, 124.0: 1726, 125.0: 1712, 126.0: 1708, 127.0: 1699, 128.0: 1688, 129.0: 1682, 130.0: 1667, 131.0: 1662, 132.0: 1660, 133.0: 1649, 134.0: 1642, 135.0: 1638, 136.0: 1629, 137.0: 1617, 138.0: 1606, 139.0: 1594, 140.0: 1590, 141.0: 1581, 142.0: 1564, 143.0: 1553, 144.0: 1545, 145.0: 1539, 146.0: 1537, 147.0: 1535, 148.0: 1527, 149.0: 1518, 150.0: 1517, 151.0: 1513, 152.0: 1508, 153.0: 1506, 154.0: 1501, 155.0: 1488, 156.0: 1474, 157.0: 1464, 158.0: 1453, 159.0: 1445, 160.0: 1437, 161.0: 1431, 162.0: 1422, 163.0: 1408, 164.0: 1396, 165.0: 1387, 166.0: 1381, 167.0: 1378, 168.0: 1374, 169.0: 1365, 170.0: 1356, 171.0: 1348, 172.0: 1338, 173.0: 1326, 174.0: 1312, 175.0: 1309, 176.0: 1306, 177.0: 1298, 178.0: 1294, 179.0: 1292, 180.0: 1278, 181.0: 1277, 182.0: 1274, 183.0: 1266, 184.0: 1258, 185.0: 1257, 186.0: 1249, 187.0: 1239, 188.0: 1224, 189.0: 1210, 190.0: 1206, 191.0: 1192, 192.0: 1187, 193.0: 1183, 194.0: 1164, 195.0: 1155, 196.0: 1144, 197.0: 1142, 198.0: 1138, 199.0: 1132, 200.0: 1130, 201.0: 1130, 202.0: 1118, 203.0: 1109, 204.0: 1103, 205.0: 1090, 206.0: 1077, 207.0: 1060, 208.0: 1053, 209.0: 1050, 210.0: 1044, 211.0: 1037, 212.0: 1024, 213.0: 1019, 214.0: 1009, 216.0: 1001, 217.0: 1000, 215.0: 998, 218.0: 986, 219.0: 976, 220.0: 958, 221.0: 947, 222.0: 938, 223.0: 937, 224.0: 937, 225.0: 929, 226.0: 916, 227.0: 911, 228.0: 902, 229.0: 894, 230.0: 891, 231.0: 885, 232.0: 876, 233.0: 867, 234.0: 860, 235.0: 855, 236.0: 850, 237.0: 847, 238.0: 839, 239.0: 836, 240.0: 828, 241.0: 824, 242.0: 819, 243.0: 808, 244.0: 798, 245.0: 795, 246.0: 789, 247.0: 783, 248.0: 775, 249.0: 767, 250.0: 753, 251.0: 741, 252.0: 736, 253.0: 719, 254.0: 707, 255.0: 702, 256.0: 693, 258.0: 687, 257.0: 686, 259.0: 679, 260.0: 668, 261.0: 661, 262.0: 656, 263.0: 647, 264.0: 645, 265.0: 638, 266.0: 631, 267.0: 619, 268.0: 610, 269.0: 601, 271.0: 600, 270.0: 599, 272.0: 593, 273.0: 592, 274.0: 580, 275.0: 570, 276.0: 560, 277.0: 553, 278.0: 541, 279.0: 537, 280.0: 534, 281.0: 527, 282.0: 519, 283.0: 514, 284.0: 508, 285.0: 499, 286.0: 489, 287.0: 485, 288.0: 479, 290.0: 478, 289.0: 472, 291.0: 467, 292.0: 461, 293.0: 458, 294.0: 452, 295.0: 445, 296.0: 437, 297.0: 433, 298.0: 423, 299.0: 417, 300.0: 415, 301.0: 404, 302.0: 394, 303.0: 389, 304.0: 372, 305.0: 365, 306.0: 364, 308.0: 360, 307.0: 359, 309.0: 355, 310.0: 340, 311.0: 330, 312.0: 318, 313.0: 307, 314.0: 306, 315.0: 302, 316.0: 298, 318.0: 293, 319.0: 290, 317.0: 289, 320.0: 280, 321.0: 276, 322.0: 271, 323.0: 265, 324.0: 253, 325.0: 247, 326.0: 239, 328.0: 228, 327.0: 226, 329.0: 222, 330.0: 212, 331.0: 209, 332.0: 199, 333.0: 195, 334.0: 185, 335.0: 173, 336.0: 166, 337.0: 166, 338.0: 165, 339.0: 159, 340.0: 150, 341.0: 136, 342.0: 131, 343.0: 125, 344.0: 108, 345.0: 96, 346.0: 89, 347.0: 83, 348.0: 82, 349.0: 80, 350.0: 78, 351.0: 69, 352.0: 62, 353.0: 54, 354.0: 51, 355.0: 45, 356.0: 35, 357.0: 32, 358.0: 21, 359.0: 20, 360.0: 16, 361.0: 13, 362.0: 9, 363.0: 6, 364.0: 3})\n"
     ]
    }
   ],
   "source": [
    "#Â Under Sample\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(return_indices=True, sampling_strategy= {1000.0:100000})\n",
    "X_rus, y_rus, id_rus = rus.fit_sample(X, y)\n",
    "print(X_rus)\n",
    "print(Counter(list(y_rus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_rus, y_rus, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1      115095.9325            7.85m\n",
      "         2      112911.6242            7.71m\n",
      "         3      110770.7826            7.75m\n",
      "         4      108672.5430            7.59m\n",
      "         5      106616.0607            7.60m\n",
      "         6      104600.4160            7.63m\n",
      "         7      102624.8815            7.65m\n",
      "         8      100688.6585            7.61m\n",
      "         9       98790.9653            7.64m\n",
      "        10       96931.0347            7.61m\n",
      "        11       95108.1218            7.61m\n",
      "        12       93321.4825            7.58m\n",
      "        13       91570.3952            7.55m\n",
      "        14       89854.1512            7.51m\n",
      "        15       88172.0500            7.46m\n",
      "        16       86523.4275            7.44m\n",
      "        17       84907.6095            7.41m\n",
      "        18       83323.9382            7.40m\n",
      "        19       81771.8179            7.40m\n",
      "        20       80250.4503            7.42m\n",
      "        21       78759.4861            7.45m\n",
      "        22       77298.0025            7.47m\n",
      "        23       75865.6524            7.50m\n",
      "        24       74461.7547            7.50m\n",
      "        25       73085.9570            7.51m\n",
      "        26       71735.1466            7.53m\n",
      "        27       70413.3541            7.52m\n",
      "        28       69115.5984            7.49m\n",
      "        29       67844.9988            7.47m\n",
      "        30       66600.2202            7.47m\n",
      "        31       65380.2109            7.50m\n",
      "        32       64184.4781            7.51m\n",
      "        33       63013.0658            7.51m\n",
      "        34       61864.3417            7.50m\n",
      "        35       60738.7327            7.49m\n",
      "        36       59633.4317            7.47m\n",
      "        37       58552.4738            7.45m\n",
      "        38       57492.3500            7.43m\n",
      "        39       56453.4002            7.42m\n",
      "        40       55435.0911            7.40m\n",
      "        41       54436.9862            7.38m\n",
      "        42       53459.2995            7.37m\n",
      "        43       52500.3525            7.36m\n",
      "        44       51560.4804            7.35m\n",
      "        45       50640.0092            7.33m\n",
      "        46       49737.1343            7.31m\n",
      "        47       48852.8219            7.29m\n",
      "        48       47985.4884            7.27m\n",
      "        49       47136.0169            7.25m\n",
      "        50       46302.8610            7.23m\n",
      "        51       45486.7013            7.22m\n",
      "        52       44684.4625            7.20m\n",
      "        53       43900.5270            7.18m\n",
      "        54       43131.6699            7.17m\n",
      "        55       42378.3024            7.16m\n",
      "        56       41637.8210            7.15m\n",
      "        57       40913.7837            7.14m\n",
      "        58       40204.7249            7.12m\n",
      "        59       39509.2516            7.11m\n",
      "        60       38825.9484            7.09m\n",
      "        61       38156.1009            7.07m\n",
      "        62       37501.3112            7.05m\n",
      "        63       36859.9660            7.03m\n",
      "        64       36229.0412            7.01m\n",
      "        65       35610.7404            6.99m\n",
      "        66       35006.3850            6.96m\n",
      "        67       34412.3936            6.94m\n",
      "        68       33831.7588            6.93m\n",
      "        69       33263.0827            6.92m\n",
      "        70       32703.7294            6.91m\n",
      "        71       32157.0451            6.89m\n",
      "        72       31621.6426            6.87m\n",
      "        73       31094.8511            6.85m\n",
      "        74       30578.5629            6.83m\n",
      "        75       30074.1070            6.81m\n",
      "        76       29578.4217            6.80m\n",
      "        77       29094.1710            6.78m\n",
      "        78       28617.9598            6.77m\n",
      "        79       28152.8127            6.75m\n",
      "        80       27696.3148            6.74m\n",
      "        81       27247.7733            6.73m\n",
      "        82       26808.2402            6.72m\n",
      "        83       26378.6997            6.69m\n",
      "        84       25957.4363            6.68m\n",
      "        85       25543.5200            6.66m\n",
      "        86       25138.0229            6.65m\n",
      "        87       24740.2292            6.64m\n",
      "        88       24351.4930            6.63m\n",
      "        89       23969.6379            6.63m\n",
      "        90       23596.1827            6.62m\n",
      "        91       23230.0766            6.61m\n",
      "        92       22870.4089            6.60m\n",
      "        93       22517.4889            6.58m\n",
      "        94       22172.8821            6.56m\n",
      "        95       21833.9870            6.55m\n",
      "        96       21502.8125            6.53m\n",
      "        97       21178.2631            6.52m\n",
      "        98       20860.2394            6.50m\n",
      "        99       20548.7005            6.48m\n",
      "       100       20242.4451            6.46m\n",
      "       101       19942.9847            6.44m\n",
      "       102       19648.8287            6.43m\n",
      "       103       19360.9937            6.41m\n",
      "       104       19077.8632            6.40m\n",
      "       105       18801.3745            6.38m\n",
      "       106       18530.3049            6.36m\n",
      "       107       18264.4675            6.34m\n",
      "       108       18003.6503            6.33m\n",
      "       109       17748.7008            6.31m\n",
      "       110       17498.6873            6.29m\n",
      "       111       17253.1816            6.27m\n",
      "       112       17012.8605            6.26m\n",
      "       113       16776.6563            6.24m\n",
      "       114       16545.6565            6.22m\n",
      "       115       16319.4449            6.20m\n",
      "       116       16097.5495            6.18m\n",
      "       117       15879.4344            6.16m\n",
      "       118       15666.4369            6.14m\n",
      "       119       15457.9477            6.12m\n",
      "       120       15253.4980            6.10m\n",
      "       121       15052.7583            6.08m\n",
      "       122       14856.1668            6.06m\n",
      "       123       14661.9827            6.05m\n",
      "       124       14472.6948            6.03m\n",
      "       125       14287.5203            6.01m\n",
      "       126       14105.8167            5.99m\n",
      "       127       13927.8861            5.98m\n",
      "       128       13753.8242            5.96m\n",
      "       129       13582.8489            5.94m\n",
      "       130       13414.9521            5.92m\n",
      "       131       13249.5308            5.90m\n",
      "       132       13087.3705            5.89m\n",
      "       133       12929.5696            5.87m\n",
      "       134       12773.1083            5.85m\n",
      "       135       12621.3984            5.84m\n",
      "       136       12472.3285            5.82m\n",
      "       137       12326.3652            5.80m\n",
      "       138       12182.1564            5.79m\n",
      "       139       12041.6219            5.77m\n",
      "       140       11904.2342            5.75m\n",
      "       141       11769.3024            5.73m\n",
      "       142       11636.8653            5.72m\n",
      "       143       11507.1943            5.70m\n",
      "       144       11380.4124            5.68m\n",
      "       145       11255.6662            5.67m\n",
      "       146       11133.4473            5.65m\n",
      "       147       11013.7686            5.64m\n",
      "       148       10896.2537            5.62m\n",
      "       149       10781.6717            5.60m\n",
      "       150       10668.8368            5.59m\n",
      "       151       10558.7229            5.57m\n",
      "       152       10449.7723            5.55m\n",
      "       153       10343.5783            5.54m\n",
      "       154       10239.1307            5.52m\n",
      "       155       10137.2735            5.51m\n",
      "       156       10037.6690            5.49m\n",
      "       157        9939.4239            5.47m\n",
      "       158        9843.3922            5.46m\n",
      "       159        9749.2222            5.44m\n",
      "       160        9656.8487            5.42m\n",
      "       161        9565.5773            5.41m\n",
      "       162        9476.9549            5.39m\n",
      "       163        9390.0033            5.38m\n",
      "       164        9305.1202            5.36m\n",
      "       165        9221.5864            5.34m\n",
      "       166        9139.6750            5.33m\n",
      "       167        9059.4994            5.31m\n",
      "       168        8980.0327            5.29m\n",
      "       169        8902.7357            5.28m\n",
      "       170        8827.0953            5.26m\n",
      "       171        8752.3382            5.25m\n",
      "       172        8678.9225            5.23m\n",
      "       173        8607.9834            5.21m\n",
      "       174        8538.0134            5.20m\n",
      "       175        8469.5281            5.18m\n",
      "       176        8402.4856            5.16m\n",
      "       177        8335.9600            5.15m\n",
      "       178        8271.4188            5.13m\n",
      "       179        8208.1757            5.11m\n",
      "       180        8146.1205            5.10m\n",
      "       181        8084.3661            5.08m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       182        8024.2420            5.07m\n",
      "       183        7965.8285            5.05m\n",
      "       184        7908.8390            5.03m\n",
      "       185        7851.7258            5.02m\n",
      "       186        7796.6184            5.00m\n",
      "       187        7742.0126            4.99m\n",
      "       188        7688.5851            4.97m\n",
      "       189        7635.7265            4.96m\n",
      "       190        7584.8145            4.94m\n",
      "       191        7534.8972            4.92m\n",
      "       192        7485.2349            4.90m\n",
      "       193        7437.2679            4.89m\n",
      "       194        7389.3058            4.87m\n",
      "       195        7342.7655            4.86m\n",
      "       196        7297.6709            4.84m\n",
      "       197        7253.3818            4.82m\n",
      "       198        7209.0207            4.81m\n",
      "       199        7166.3816            4.79m\n",
      "       200        7124.7240            4.78m\n",
      "       201        7083.6875            4.76m\n",
      "       202        7042.8925            4.75m\n",
      "       203        7002.4124            4.73m\n",
      "       204        6962.8873            4.72m\n",
      "       205        6924.3943            4.70m\n",
      "       206        6886.3887            4.68m\n",
      "       207        6850.0426            4.67m\n",
      "       208        6813.7545            4.65m\n",
      "       209        6778.6940            4.63m\n",
      "       210        6743.5359            4.62m\n",
      "       211        6709.0424            4.60m\n",
      "       212        6675.3234            4.59m\n",
      "       213        6642.1461            4.57m\n",
      "       214        6610.2338            4.55m\n",
      "       215        6579.2135            4.54m\n",
      "       216        6547.8993            4.52m\n",
      "       217        6517.1759            4.50m\n",
      "       218        6487.5719            4.49m\n",
      "       219        6458.8480            4.47m\n",
      "       220        6430.4248            4.45m\n",
      "       221        6401.9897            4.44m\n",
      "       222        6374.3093            4.42m\n",
      "       223        6347.3391            4.40m\n",
      "       224        6321.0017            4.39m\n",
      "       225        6294.6590            4.37m\n",
      "       226        6268.8524            4.35m\n",
      "       227        6243.5065            4.34m\n",
      "       228        6218.9263            4.32m\n",
      "       229        6195.2614            4.31m\n",
      "       230        6171.7025            4.29m\n",
      "       231        6148.2204            4.28m\n",
      "       232        6125.9116            4.26m\n",
      "       233        6104.1393            4.24m\n",
      "       234        6082.1369            4.23m\n",
      "       235        6060.3179            4.21m\n",
      "       236        6039.2293            4.19m\n",
      "       237        6018.2715            4.18m\n",
      "       238        5998.4241            4.16m\n",
      "       239        5978.1848            4.15m\n",
      "       240        5958.3933            4.13m\n",
      "       241        5938.9424            4.12m\n",
      "       242        5920.1555            4.10m\n",
      "       243        5901.9568            4.08m\n",
      "       244        5883.6104            4.07m\n",
      "       245        5865.2012            4.05m\n",
      "       246        5848.3602            4.03m\n",
      "       247        5830.6262            4.02m\n",
      "       248        5813.0584            4.00m\n",
      "       249        5796.1572            3.99m\n",
      "       250        5779.7845            3.97m\n",
      "       251        5763.3375            3.96m\n",
      "       252        5747.7468            3.94m\n",
      "       253        5732.0499            3.93m\n",
      "       254        5716.8450            3.91m\n",
      "       255        5702.1830            3.89m\n",
      "       256        5686.9971            3.88m\n",
      "       257        5672.6321            3.86m\n",
      "       258        5658.5279            3.84m\n",
      "       259        5644.3159            3.83m\n",
      "       260        5630.2109            3.81m\n",
      "       261        5616.9081            3.79m\n",
      "       262        5603.6266            3.78m\n",
      "       263        5590.4220            3.76m\n",
      "       264        5578.3426            3.75m\n",
      "       265        5565.4623            3.73m\n",
      "       266        5552.7673            3.71m\n",
      "       267        5540.3284            3.70m\n",
      "       268        5527.9726            3.68m\n",
      "       269        5515.8528            3.67m\n",
      "       270        5505.3718            3.65m\n",
      "       271        5493.9116            3.63m\n",
      "       272        5482.1832            3.62m\n",
      "       273        5471.9246            3.60m\n",
      "       274        5461.8952            3.58m\n",
      "       275        5450.7804            3.57m\n",
      "       276        5440.7320            3.55m\n",
      "       277        5431.4301            3.53m\n",
      "       278        5420.9068            3.52m\n",
      "       279        5411.3349            3.50m\n",
      "       280        5401.1625            3.49m\n",
      "       281        5392.1622            3.47m\n",
      "       282        5382.7930            3.45m\n",
      "       283        5374.1840            3.44m\n",
      "       284        5366.1353            3.42m\n",
      "       285        5356.7558            3.41m\n",
      "       286        5348.6447            3.39m\n",
      "       287        5340.9905            3.37m\n",
      "       288        5333.0368            3.36m\n",
      "       289        5324.4801            3.34m\n",
      "       290        5316.7747            3.32m\n",
      "       291        5309.2472            3.31m\n",
      "       292        5301.0961            3.29m\n",
      "       293        5293.8216            3.28m\n",
      "       294        5285.7476            3.26m\n",
      "       295        5278.7334            3.25m\n",
      "       296        5271.7595            3.23m\n",
      "       297        5264.8877            3.21m\n",
      "       298        5258.0698            3.20m\n",
      "       299        5250.8046            3.18m\n",
      "       300        5244.8804            3.17m\n",
      "       301        5237.8686            3.15m\n",
      "       302        5231.7174            3.13m\n",
      "       303        5224.6953            3.12m\n",
      "       304        5218.7154            3.10m\n",
      "       305        5212.1522            3.09m\n",
      "       306        5205.0378            3.07m\n",
      "       307        5199.0123            3.05m\n",
      "       308        5193.0322            3.04m\n",
      "       309        5187.7720            3.02m\n",
      "       310        5182.8832            3.01m\n",
      "       311        5177.4370            2.99m\n",
      "       312        5171.1872            2.97m\n",
      "       313        5165.8008            2.96m\n",
      "       314        5160.2293            2.94m\n",
      "       315        5155.1155            2.92m\n",
      "       316        5149.9655            2.91m\n",
      "       317        5144.7808            2.89m\n",
      "       318        5140.0825            2.88m\n",
      "       319        5135.0342            2.86m\n",
      "       320        5130.8095            2.84m\n",
      "       321        5126.5007            2.83m\n",
      "       322        5121.2598            2.81m\n",
      "       323        5116.5474            2.79m\n",
      "       324        5112.2369            2.78m\n",
      "       325        5107.9834            2.76m\n",
      "       326        5103.3226            2.75m\n",
      "       327        5099.4275            2.73m\n",
      "       328        5095.1323            2.71m\n",
      "       329        5091.1220            2.70m\n",
      "       330        5086.9288            2.68m\n",
      "       331        5083.2801            2.67m\n",
      "       332        5078.6830            2.65m\n",
      "       333        5075.4177            2.63m\n",
      "       334        5071.7470            2.62m\n",
      "       335        5067.8880            2.60m\n",
      "       336        5064.8446            2.58m\n",
      "       337        5060.5732            2.57m\n",
      "       338        5057.0265            2.55m\n",
      "       339        5053.7157            2.54m\n",
      "       340        5049.9125            2.52m\n",
      "       341        5046.3545            2.50m\n",
      "       342        5042.3697            2.49m\n",
      "       343        5039.1023            2.47m\n",
      "       344        5036.3865            2.46m\n",
      "       345        5032.9497            2.44m\n",
      "       346        5029.5076            2.42m\n",
      "       347        5026.4415            2.41m\n",
      "       348        5023.7655            2.39m\n",
      "       349        5021.2631            2.38m\n",
      "       350        5018.3206            2.36m\n",
      "       351        5015.1530            2.34m\n",
      "       352        5011.6127            2.33m\n",
      "       353        5009.1878            2.31m\n",
      "       354        5005.5194            2.30m\n",
      "       355        5002.5527            2.28m\n",
      "       356        4999.7373            2.26m\n",
      "       357        4996.9041            2.25m\n",
      "       358        4994.7439            2.23m\n",
      "       359        4992.2184            2.22m\n",
      "       360        4989.6439            2.20m\n",
      "       361        4987.1585            2.18m\n",
      "       362        4985.0976            2.17m\n",
      "       363        4983.2785            2.15m\n",
      "       364        4981.2635            2.14m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       365        4978.8980            2.12m\n",
      "       366        4977.1728            2.10m\n",
      "       367        4974.7385            2.09m\n",
      "       368        4972.7930            2.07m\n",
      "       369        4970.6709            2.06m\n",
      "       370        4968.5960            2.04m\n",
      "       371        4966.3388            2.02m\n",
      "       372        4964.6084            2.01m\n",
      "       373        4962.5619            1.99m\n",
      "       374        4960.6454            1.98m\n",
      "       375        4958.1013            1.96m\n",
      "       376        4955.9507            1.94m\n",
      "       377        4954.3594            1.93m\n",
      "       378        4952.2097            1.91m\n",
      "       379        4950.0887            1.90m\n",
      "       380        4948.4244            1.88m\n",
      "       381        4946.8689            1.86m\n",
      "       382        4945.1655            1.85m\n",
      "       383        4943.2291            1.83m\n",
      "       384        4941.7979            1.82m\n",
      "       385        4940.3249            1.80m\n",
      "       386        4938.6692            1.78m\n",
      "       387        4937.2488            1.77m\n",
      "       388        4935.7696            1.75m\n",
      "       389        4933.8593            1.74m\n",
      "       390        4931.8013            1.72m\n",
      "       391        4929.9108            1.70m\n",
      "       392        4928.8086            1.69m\n",
      "       393        4927.4930            1.67m\n",
      "       394        4925.8336            1.66m\n",
      "       395        4924.4101            1.64m\n",
      "       396        4922.7218            1.62m\n",
      "       397        4921.1424            1.61m\n",
      "       398        4919.6643            1.59m\n",
      "       399        4918.4889            1.58m\n",
      "       400        4916.9236            1.56m\n",
      "       401        4915.1730            1.55m\n",
      "       402        4913.9321            1.53m\n",
      "       403        4912.4408            1.51m\n",
      "       404        4911.3082            1.50m\n",
      "       405        4910.1937            1.48m\n",
      "       406        4908.5611            1.47m\n",
      "       407        4907.0729            1.45m\n",
      "       408        4906.0221            1.43m\n",
      "       409        4904.4363            1.42m\n",
      "       410        4902.7820            1.40m\n",
      "       411        4900.5508            1.39m\n",
      "       412        4899.4340            1.37m\n",
      "       413        4898.0778            1.36m\n",
      "       414        4897.0417            1.34m\n",
      "       415        4896.0303            1.32m\n",
      "       416        4894.7101            1.31m\n",
      "       417        4893.5519            1.29m\n",
      "       418        4891.9211            1.28m\n",
      "       419        4890.5167            1.26m\n",
      "       420        4889.5588            1.25m\n",
      "       421        4888.6217            1.23m\n",
      "       422        4887.8403            1.21m\n",
      "       423        4886.3643            1.20m\n",
      "       424        4884.6352            1.18m\n",
      "       425        4883.2803            1.17m\n",
      "       426        4882.4663            1.15m\n",
      "       427        4881.2813            1.14m\n",
      "       428        4879.5839            1.12m\n",
      "       429        4878.7129            1.10m\n",
      "       430        4877.5739            1.09m\n",
      "       431        4876.8593            1.07m\n",
      "       432        4875.5151            1.06m\n",
      "       433        4874.4061            1.04m\n",
      "       434        4873.1641            1.03m\n",
      "       435        4871.5396            1.01m\n",
      "       436        4870.2923           59.68s\n",
      "       437        4869.2243           58.74s\n",
      "       438        4868.4863           57.80s\n",
      "       439        4867.7206           56.87s\n",
      "       440        4866.9677           55.91s\n",
      "       441        4865.8960           54.98s\n",
      "       442        4864.3390           54.06s\n",
      "       443        4863.2668           53.12s\n",
      "       444        4862.0577           52.18s\n",
      "       445        4861.2731           51.23s\n",
      "       446        4860.1047           50.31s\n",
      "       447        4858.5280           49.38s\n",
      "       448        4857.6147           48.45s\n",
      "       449        4855.7702           47.51s\n",
      "       450        4855.1005           46.57s\n",
      "       451        4854.0033           45.62s\n",
      "       452        4852.6682           44.69s\n",
      "       453        4851.9983           43.75s\n",
      "       454        4851.3744           42.82s\n",
      "       455        4850.7152           41.89s\n",
      "       456        4849.6812           40.96s\n",
      "       457        4848.8117           40.02s\n",
      "       458        4847.1173           39.12s\n",
      "       459        4845.3218           38.21s\n",
      "       460        4843.6734           37.30s\n",
      "       461        4841.9290           36.39s\n",
      "       462        4840.3856           35.48s\n",
      "       463        4837.9482           34.57s\n",
      "       464        4836.6781           33.66s\n",
      "       465        4834.1703           32.74s\n",
      "       466        4832.6231           31.83s\n",
      "       467        4831.1343           30.91s\n",
      "       468        4829.9629           29.99s\n",
      "       469        4827.6183           29.07s\n",
      "       470        4825.9996           28.15s\n",
      "       471        4824.5728           27.23s\n",
      "       472        4822.9961           26.31s\n",
      "       473        4821.6691           25.39s\n",
      "       474        4819.6746           24.46s\n",
      "       475        4818.1443           23.53s\n",
      "       476        4816.8560           22.60s\n",
      "       477        4814.4846           21.67s\n",
      "       478        4813.4186           20.74s\n",
      "       479        4811.9353           19.81s\n",
      "       480        4809.7690           18.87s\n",
      "       481        4808.4546           17.94s\n",
      "       482        4807.1496           17.00s\n",
      "       483        4804.9382           16.07s\n",
      "       484        4802.7767           15.13s\n",
      "       485        4802.0691           14.19s\n",
      "       486        4800.2055           13.25s\n",
      "       487        4799.0279           12.31s\n",
      "       488        4797.6597           11.37s\n",
      "       489        4795.4691           10.42s\n",
      "       490        4794.5548            9.48s\n",
      "       491        4793.7281            8.54s\n",
      "       492        4791.6162            7.59s\n",
      "       493        4790.7659            6.65s\n",
      "       494        4790.1018            5.70s\n",
      "       495        4788.2201            4.75s\n",
      "       496        4787.6102            3.80s\n",
      "       497        4785.7175            2.85s\n",
      "       498        4783.7263            1.90s\n",
      "       499        4782.3510            0.95s\n",
      "       500        4780.4023            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.01, loss='ls', max_depth=6,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=100,\n",
       "                          warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train GBR\n",
    "from sklearn import ensemble\n",
    "params = {'n_estimators': 500, 'max_depth': 6, \n",
    "          'learning_rate': 0.01,'verbose': 100}\n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "print('start')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.33680930865866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['regressor1.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "print(mean_absolute_error(pred, y_test))\n",
    "dump(clf, 'regressor2.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.41319049   2.68628135   3.00213487   3.69693804   3.5771524\n",
      " 123.84047095   3.14612935   3.20980367   3.50713016 128.59656693]\n",
      "[ 0.  0.  0.  0.  0. 81.  0.  0.  0. 58.]\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Predict and print metrics\n",
    "regressor = load('regressor.joblib')\n",
    "p = regressor.predict(X_test)\n",
    "print(mean_absolute_error(p, y_test))\n",
    "print(p[:10])\n",
    "print(y_test[:10])\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(p, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523190,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-c2d21e35afdd4e62a4e78ad197b42fae.pkl\n",
      "Pickling array (shape=(58133,), dtype=int32).\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523190,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-92b88670b4144144a08b9df6a0f1af02.pkl\n",
      "Pickling array (shape=(58133,), dtype=int32).\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523190,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-9b6a3062bc9345e88d3807522b881838.pkl\n",
      "Pickling array (shape=(58133,), dtype=int32).\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523191,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-5a31f17130c84991828edb72656787b3.pkl\n",
      "Pickling array (shape=(58132,), dtype=int32).\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523191,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-958f65d3ad744f3c8c54b0c5b48b3cb6.pkl\n",
      "Pickling array (shape=(58132,), dtype=int32).\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523191,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-6bb25806024d4d72ade40c9d00c3beb7.pkl\n",
      "Pickling array (shape=(58132,), dtype=int32).\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523191,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-fa1a9e04ae084c4c83dbba54c64c6633.pkl\n",
      "Pickling array (shape=(58132,), dtype=int32).\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523191,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-3018126a236142639d9aaba9b8f4c0da.pkl\n",
      "Pickling array (shape=(58132,), dtype=int32).\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523191,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-0ad1bb938eb641f28c62099480d74ff1.pkl\n",
      "Pickling array (shape=(58132,), dtype=int32).\n",
      "Memmapping (shape=(581323, 10), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-d0d84e85f50e496e8c337d8863538e3b.pkl\n",
      "Memmapping (shape=(581323,), dtype=float64) to old file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-685116f82b194b5a8fdf26f23bdd5daa.pkl\n",
      "Memmapping (shape=(523191,), dtype=int32) to new file C:\\Users\\admin\\AppData\\Local\\Temp\\joblib_memmapping_folder_15644_2346536603\\15644-2803034264520-913e93512a0c4a61bd6b5312a7d19afd.pkl\n",
      "Pickling array (shape=(58132,), dtype=int32).\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.datasets import load_digits \n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Start\")\n",
    "\n",
    "def GradientBooster(param_grid):\n",
    "    estimator = ensemble.GradientBoostingRegressor()\n",
    "    \n",
    "    #cv = ShuffleSplit(X_train.shape[0], n_iter=10, test_size=0.2) \n",
    "\n",
    "    classifier = GridSearchCV(estimator=estimator, cv=10, param_grid=param_grid, n_jobs=-1, verbose=100) \n",
    "    classifier.fit(X_rus, y_rus)\n",
    "    \n",
    "    print (classifier.best_estimator_ )\n",
    "    return classifier.best_estimator_\n",
    "\n",
    "param_grid={'n_estimators':[500], \n",
    "            'learning_rate': [0.1],\n",
    "            'max_depth':[10], \n",
    "            'min_samples_leaf':[1]\n",
    "           }\n",
    "\n",
    "#Let's fit GBRT to the digits training dataset by calling the function we just created. \n",
    "best_est = GradientBooster(param_grid)\n",
    "\n",
    "\n",
    "\n",
    "#OK great, so we got back the best estimator parameters as follows:\n",
    "print (\"Best Estimator Parameters\")\n",
    "print (\"---------------------------\")\n",
    "print (\"n_estimators: %d\" %best_est.n_estimators)\n",
    "print (\"max_depth: %d\" %best_est.max_depth)\n",
    "print (\"Learning Rate: %.1f\" %best_est.learning_rate)\n",
    "print (\"min_samples_leaf: %d\" %best_est.min_samples_leaf)\n",
    "print (\"max_features: %.1f\" %best_est.max_features)\n",
    "\n",
    "print()\n",
    "print (\"Train R-squared: %.2f\" %best_est.score(X_train,y_train))\n",
    "\"\"\"\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)): \n",
    "    plt.figure() \n",
    "    plt.title(title) \n",
    "    if ylim is not None: \n",
    "        plt.ylim(*ylim) \n",
    "        plt.xlabel(\"Training examples\") \n",
    "        plt.ylabel(\"Score\") \n",
    "        train_sizes, train_scores, test_scores = learning_curve( estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes) \n",
    "        train_scores_mean = np.mean(train_scores, axis=1) \n",
    "        train_scores_std = np.std(train_scores, axis=1) \n",
    "        test_scores_mean = np.mean(test_scores, axis=1) \n",
    "        test_scores_std = np.std(test_scores, axis=1) \n",
    "        plt.grid() \n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\") \n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\") \n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\") \n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\") \n",
    "        plt.legend(loc=\"best\") \n",
    "        return plt\"\"\"\n",
    "\"\"\"\n",
    "estimator = ensemble.GradientBoostingRegressor(n_estimators=best_est.n_estimators, \n",
    "                                      max_depth=best_est.max_depth, \n",
    "                                      learning_rate=best_est.learning_rate, \n",
    "                                      min_samples_leaf=best_est.min_samples_leaf, \n",
    "                                      max_features=best_est.max_features,\n",
    "                                      n_jobs=-1,\n",
    "                                     verbose = 100) \n",
    "\n",
    "estimator.fit(X, y)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1       75495.8468            2.58m\n",
      "         2       61318.0277            2.63m\n",
      "         3       49833.6919            2.57m\n",
      "         4       40527.5781            2.54m\n",
      "         5       32991.4663            2.50m\n",
      "         6       26886.7316            2.47m\n",
      "         7       21941.6387            2.46m\n",
      "         8       17936.2354            2.43m\n",
      "         9       14688.7802            2.42m\n",
      "        10       12060.7491            2.40m\n",
      "        11        9930.6896            2.37m\n",
      "        12        8205.4544            2.34m\n",
      "        13        6806.7486            2.32m\n",
      "        14        5673.0557            2.29m\n",
      "        15        4755.5161            2.26m\n",
      "        16        4011.3288            2.23m\n",
      "        17        3407.6989            2.21m\n",
      "        18        2919.2306            2.18m\n",
      "        19        2523.6074            2.16m\n",
      "        20        2200.4420            2.13m\n",
      "        21        1940.3042            2.10m\n",
      "        22        1729.6436            2.08m\n",
      "        23        1556.2213            2.06m\n",
      "        24        1416.1442            2.03m\n",
      "        25        1301.9439            2.00m\n",
      "        26        1208.5390            1.98m\n",
      "        27        1132.1646            1.95m\n",
      "        28        1072.3154            1.92m\n",
      "        29        1021.6819            1.90m\n",
      "        30         981.3020            1.87m\n",
      "        31         947.1106            1.85m\n",
      "        32         919.6009            1.82m\n",
      "        33         895.0444            1.79m\n",
      "        34         876.8960            1.76m\n",
      "        35         860.3944            1.73m\n",
      "        36         848.5776            1.70m\n",
      "        37         838.7548            1.67m\n",
      "        38         828.6968            1.65m\n",
      "        39         820.6222            1.63m\n",
      "        40         815.1429            1.60m\n",
      "        41         810.5773            1.57m\n",
      "        42         806.1987            1.54m\n",
      "        43         801.8047            1.52m\n",
      "        44         796.4804            1.52m\n",
      "        45         793.3879            1.51m\n",
      "        46         791.3682            1.49m\n",
      "        47         789.0482            1.48m\n",
      "        48         786.9721            1.47m\n",
      "        49         784.2351            1.46m\n",
      "        50         781.3276            1.45m\n",
      "        51         779.3847            1.44m\n",
      "        52         777.6048            1.43m\n",
      "        53         775.1309            1.41m\n",
      "        54         774.2547            1.39m\n",
      "        55         770.5979            1.38m\n",
      "        56         767.7957            1.36m\n",
      "        57         762.9741            1.35m\n",
      "        58         759.0615            1.33m\n",
      "        59         755.1501            1.31m\n",
      "        60         751.9338            1.30m\n",
      "        61         749.5280            1.28m\n",
      "        62         747.0404            1.26m\n",
      "        63         744.5430            1.23m\n",
      "        64         741.8278            1.21m\n",
      "        65         739.2761            1.19m\n",
      "        66         736.0319            1.16m\n",
      "        67         733.9033            1.14m\n",
      "        68         731.2087            1.11m\n",
      "        69         729.6435            1.07m\n",
      "        70         728.2438            1.04m\n",
      "        71         725.0296            1.01m\n",
      "        72         723.4955           58.79s\n",
      "        73         722.6419           56.63s\n",
      "        74         720.9393           54.82s\n",
      "        75         720.3055           52.66s\n",
      "        76         719.9485           50.48s\n",
      "        77         718.4119           48.57s\n",
      "        78         716.4492           46.72s\n",
      "        79         715.1003           44.77s\n",
      "        80         713.4050           42.82s\n",
      "        81         710.7295           40.90s\n",
      "        82         708.9191           38.93s\n",
      "        83         706.5974           36.96s\n",
      "        84         705.3987           34.92s\n",
      "        85         703.8266           32.88s\n",
      "        86         702.9471           30.76s\n",
      "        87         702.2432           28.54s\n",
      "        88         700.2268           26.47s\n",
      "        89         698.9595           24.34s\n",
      "        90         696.4687           22.22s\n",
      "        91         695.5092           20.05s\n",
      "        92         694.9956           17.81s\n",
      "        93         693.7684           15.63s\n",
      "        94         692.9198           13.41s\n",
      "        95         692.5757           11.19s\n",
      "        96         690.8772            8.99s\n",
      "        97         689.5160            6.76s\n",
      "        98         688.7430            4.52s\n",
      "        99         688.1539            2.26s\n",
      "       100         687.7111            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=6,\n",
       "                          max_features=1.0, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=3, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=100,\n",
       "                          warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "estimator = ensemble.GradientBoostingRegressor(n_estimators=100, \n",
    "                                      max_depth=6, \n",
    "                                      learning_rate=0.1, \n",
    "                                      min_samples_leaf=3, \n",
    "                                      max_features=1.0,\n",
    "                                     verbose = 100) \n",
    "\n",
    "estimator.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "dump(estimator, 'crossvalidation1.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.870788375532486\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "est = load('crossvalidation.joblib')\n",
    "predictioncv = est.predict(X_test)\n",
    "print(mean_absolute_error(predictioncv, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[999.98269128 999.98269128 999.98269128 999.98269128 999.98269128\n",
      " 999.98269128 999.98269128 999.98269128 100.77974751 999.98269128]\n",
      "[1000. 1000. 1000. 1000. 1000. 1000. 1000. 1000.  151. 1000.]\n"
     ]
    }
   ],
   "source": [
    "print(predictioncv[10:20])\n",
    "print(y_test[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9924888509832172"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(predictioncv, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
